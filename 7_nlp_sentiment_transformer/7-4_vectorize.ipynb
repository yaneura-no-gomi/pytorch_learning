{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 単語分割にはMecab＋NEologdを使用\n",
    "import MeCab\n",
    "\n",
    "m_t = MeCab.Tagger('-Owakati -d /usr/lib/x86_64-linux-gnu/mecab/dic/mecab-ipadic-neologd')\n",
    "\n",
    "def tokenizer_mecab(text):\n",
    "    text = m_t.parse(text)  # これでスペースで単語が区切られる\n",
    "    ret = text.strip().split()  # スペース部分で区切ったリストに変換\n",
    "    return ret\n",
    "\n",
    "\n",
    "\n",
    "# 前処理として正規化をする関数を定義\n",
    "import re\n",
    "\n",
    "def preprocessing_text(text):\n",
    "    # 改行、半角スペース、全角スペースを削除\n",
    "    text = re.sub('\\r', '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub('　', '', text)\n",
    "    text = re.sub(' ', '', text)\n",
    "\n",
    "    # 数字文字の一律「0」化\n",
    "    text = re.sub(r'[0-9 ０-９]', '0', text)  # 数字\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "# 前処理とJanomeの単語分割を合わせた関数を定義する\n",
    "\n",
    "\n",
    "def tokenizer_with_preprocessing(text):\n",
    "    text = preprocessing_text(text)  # 前処理の正規化\n",
    "    ret = tokenizer_mecab(text)  # Mecabの単語分割\n",
    "\n",
    "    return ret\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext\n",
    "\n",
    "# tsvやcsvデータを読み込んだときに、読み込んだ内容に対して行う処理を定義します\n",
    "# 文章とラベルの両方に用意します\n",
    "\n",
    "max_length = 25\n",
    "TEXT = torchtext.data.Field(sequential=True, tokenize=tokenizer_with_preprocessing,\n",
    "                            use_vocab=True, lower=True, include_lengths=True, batch_first=True, fix_length=max_length)\n",
    "LABEL = torchtext.data.Field(sequential=False, use_vocab=False)\n",
    "\n",
    "\n",
    "# フォルダ「data」から各tsvファイルを読み込みます\n",
    "train_ds, val_ds, test_ds = torchtext.data.TabularDataset.splits(\n",
    "    path='./data/', train='text_train.tsv',\n",
    "    validation='text_val.tsv', test='text_test.tsv', format='tsv',\n",
    "    fields=[('Text', TEXT), ('Label', LABEL)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "\n",
    "# 一度gensimライブラリで読み込んで、word2vecのformatで保存する\n",
    "model = KeyedVectors.load_word2vec_format(\n",
    "    './data/entity_vector/entity_vector.model.bin', binary=True)\n",
    "\n",
    "# 保存（時間がかかります、10分弱）\n",
    "model.wv.save_word2vec_format('./data/japanese_word2vec_vectors.vec')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torchtextで単語ベクトルとして読み込みます\n",
    "from torchtext.vocab import Vectors\n",
    "\n",
    "japanese_word2vec_vectors = Vectors(\n",
    "    name='./data/japanese_word2vec_vectors.vec')\n",
    "\n",
    "# 単語ベクトルの中身を確認します\n",
    "print(\"1単語を表現する次元数：\", japanese_word2vec_vectors.dim)\n",
    "print(\"単語数：\", len(japanese_word2vec_vectors.itos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ベクトル化したバージョンのボキャブラリーを作成します\n",
    "TEXT.build_vocab(train_ds, vectors=japanese_word2vec_vectors, min_freq=1)\n",
    "\n",
    "# ボキャブラリーのベクトルを確認します\n",
    "print(TEXT.vocab.vectors.shape)  # 49個の単語が200次元のベクトルで表現されている\n",
    "TEXT.vocab.vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.vocab.stoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 姫 - 女性 + 男性 のベクトルがどれと似ているのか確認してみます\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 姫 - 女性 + 男性\n",
    "tensor_calc = TEXT.vocab.vectors[41] - \\\n",
    "    TEXT.vocab.vectors[38] + TEXT.vocab.vectors[46]\n",
    "\n",
    "# コサイン類似度を計算\n",
    "# dim=0 は0次元目で計算してくださいという指定\n",
    "print(\"女王\", F.cosine_similarity(tensor_calc, TEXT.vocab.vectors[39], dim=0))\n",
    "print(\"王\", F.cosine_similarity(tensor_calc, TEXT.vocab.vectors[44], dim=0))\n",
    "print(\"王子\", F.cosine_similarity(tensor_calc, TEXT.vocab.vectors[45], dim=0))\n",
    "print(\"機械学習\", F.cosine_similarity(tensor_calc, TEXT.vocab.vectors[43], dim=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# torchtextで単語ベクトルとして読み込みます\n",
    "# word2vecとは異なり、すぐに読み込めます\n",
    "\n",
    "from torchtext.vocab import Vectors\n",
    "\n",
    "japanese_fasttext_vectors = Vectors(name='./data/vector_neologd/model.vec')\n",
    "\n",
    "                                    \n",
    "# 単語ベクトルの中身を確認します\n",
    "print(\"1単語を表現する次元数：\", japanese_fasttext_vectors.dim)\n",
    "print(\"単語数：\", len(japanese_fasttext_vectors.itos))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ベクトル化したバージョンのボキャブラリーを作成します\n",
    "TEXT.build_vocab(train_ds, vectors=japanese_fasttext_vectors, min_freq=1)\n",
    "\n",
    "# ボキャブラリーのベクトルを確認します\n",
    "print(TEXT.vocab.vectors.shape)  # 52個の単語が300次元のベクトルで表現されている\n",
    "TEXT.vocab.vectors\n",
    "\n",
    "# ボキャブラリーの単語の順番を確認します\n",
    "TEXT.vocab.stoi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 姫 - 女性 + 男性 のベクトルがどれと似ているのか確認してみます\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 姫 - 女性 + 男性\n",
    "tensor_calc = TEXT.vocab.vectors[41] - \\\n",
    "    TEXT.vocab.vectors[38] + TEXT.vocab.vectors[46]\n",
    "\n",
    "# コサイン類似度を計算\n",
    "# dim=0 は0次元目で計算してくださいという指定\n",
    "print(\"女王\", F.cosine_similarity(tensor_calc, TEXT.vocab.vectors[39], dim=0))\n",
    "print(\"王\", F.cosine_similarity(tensor_calc, TEXT.vocab.vectors[44], dim=0))\n",
    "print(\"王子\", F.cosine_similarity(tensor_calc, TEXT.vocab.vectors[45], dim=0))\n",
    "print(\"機械学習\", F.cosine_similarity(tensor_calc, TEXT.vocab.vectors[43], dim=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
